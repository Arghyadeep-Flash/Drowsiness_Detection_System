{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eye_Blink.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2bvySKsgPkpFTKpbJIi7a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arghyadeep-Flash/Drowsiness_Detection_System/blob/main/Eye_Blink.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0X2zcC3mpT2"
      },
      "source": [
        "# **Mounting Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcKKRUEfl7hL",
        "outputId": "3452af82-8cbe-4eba-c849-02e22f96e784"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W1ES99CmvYd"
      },
      "source": [
        "from scipy.spatial import distance as dist\n",
        "from imutils.video import FileVideoStream\n",
        "from imutils.video import VideoStream\n",
        "from imutils import face_utils\n",
        "import numpy as np\n",
        "import imutils\n",
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "import requests"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZroOKR1dm4zd"
      },
      "source": [
        "# **Calculate Eye Aspect Ratio**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtrKy-Mgm9Ct"
      },
      "source": [
        "def eye_aspect_ratio(eye):\n",
        "    # compute the euclidean distances between the two sets of\n",
        "    # vertical eye landmarks (x, y)-coordinates\n",
        "    A = dist.euclidean(eye[1], eye[5])\n",
        "    B = dist.euclidean(eye[2], eye[4])\n",
        "\n",
        "    # compute the euclidean distance between the horizontal\n",
        "    # eye landmark (x, y)-coordinates\n",
        "    C = dist.euclidean(eye[0], eye[3])\n",
        "\n",
        "    # compute the eye aspect ratio\n",
        "    ear = (A + B) / (2.0 * C)\n",
        "\n",
        "    # return the eye aspect ratio\n",
        "    return ear"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Nqp0uQnSWe"
      },
      "source": [
        "# **Define variable and Thresholds**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPtDkVvynWCN"
      },
      "source": [
        "# define two constants, one for the eye aspect ratio to indicate\n",
        "# blink and then a second constant for the number of consecutive\n",
        "# frames the eye must be below the threshold\n",
        "EYE_AR_THRESH = 0.3\n",
        "EYE_AR_CONSEC_FRAMES = 3\n",
        "\n",
        "# initialize the frame counters and the total number of blinks\n",
        "COUNTER = 0\n",
        "TOTAL = 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUqdFN0tneCI"
      },
      "source": [
        "# **Face Detection and Shape Predictor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhUUZbjhnicm",
        "outputId": "dc0d7ea4-86f0-448a-e9a7-9b1f467d40db"
      },
      "source": [
        "# initialize dlib's face detector (HOG-based) and then create\n",
        "# the facial landmark predictor\n",
        "print(\"[INFO] loading facial landmark predictor...\")\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"/content/drive/MyDrive/Colab Notebooks/shape_predictor_68_face_landmarks.dat\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading facial landmark predictor...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciwioyAjnvKe"
      },
      "source": [
        "# **Grab left and right eye indices**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuKVF9aKn1ae"
      },
      "source": [
        "# grab the indexes of the facial landmarks for the left and\n",
        "# right eye, respectively\n",
        "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
        "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUyY8zKbn5_O"
      },
      "source": [
        "# **Video Stream**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "I_V9ijfOn86W",
        "outputId": "feec37a8-9924-4f3b-cc30-6478f328f606"
      },
      "source": [
        "# start the video stream thread\n",
        "print(\"[INFO] starting video stream thread...\")\n",
        "\n",
        "# ------- for detection via video\n",
        "# vs = FileVideoStream(args[\"video\"]).start()\n",
        "# fileStream = True\n",
        "\n",
        "# ------- for live detection\n",
        "vs = VideoStream(src=0).start()\n",
        "#vs = VideoStream(usePiCamera=False).start()\n",
        "fileStream = False\n",
        "\n",
        "\n",
        "time.sleep(1.0)\n",
        "\n",
        "# loop over frames from the video stream\n",
        "while True:\n",
        "    # if this is a file video stream, then we need to check if\n",
        "    # there any more frames left in the buffer to process\n",
        "    if fileStream and not vs.more():\n",
        "        break\n",
        "\n",
        "    # grab the frame from the threaded video file stream, resize\n",
        "    # it, and convert it to grayscale\n",
        "    # channels)\n",
        "    frame = vs.read()\n",
        "    frame = imutils.resize(frame, width=450)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # detect faces in the grayscale frame\n",
        "    rects = detector(gray, 0)\n",
        "\n",
        "    # loop over the face detections\n",
        "    for rect in rects:\n",
        "        # determine the facial landmarks for the face region, then\n",
        "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
        "        # array\n",
        "        shape = predictor(gray, rect)\n",
        "        shape = face_utils.shape_to_np(shape)\n",
        "\n",
        "        # extract the left and right eye coordinates, then use the\n",
        "        # coordinates to compute the eye aspect ratio for both eyes\n",
        "        leftEye = shape[lStart:lEnd]\n",
        "        rightEye = shape[rStart:rEnd]\n",
        "        leftEAR = eye_aspect_ratio(leftEye)\n",
        "        rightEAR = eye_aspect_ratio(rightEye)\n",
        "\n",
        "        # average the eye aspect ratio together for both eyes\n",
        "        ear = (leftEAR + rightEAR) / 2.0\n",
        "\n",
        "        # compute the convex hull for the left and right eye, then\n",
        "        # visualize each of the eyes\n",
        "        leftEyeHull = cv2.convexHull(leftEye)\n",
        "        rightEyeHull = cv2.convexHull(rightEye)\n",
        "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
        "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
        "\n",
        "        # check to see if the eye aspect ratio is below the blink\n",
        "        # threshold, and if so, increment the blink frame counter\n",
        "        if ear < EYE_AR_THRESH:\n",
        "            COUNTER += 1\n",
        "\n",
        "        # otherwise, the eye aspect ratio is not below the blink\n",
        "        # threshold\n",
        "        else:\n",
        "            # if the eyes were closed for a sufficient number of\n",
        "            # then increment the total number of blinks\n",
        "            if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
        "                TOTAL += 1\n",
        "\n",
        "            # reset the eye frame counter\n",
        "            COUNTER = 0\n",
        "\n",
        "        # draw the total number of blinks on the frame along with\n",
        "        # the computed eye aspect ratio for the frame\n",
        "        cv2.putText(frame, \"Blinks: {}\".format(TOTAL), (10, 30),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        \n",
        "        if TOTAL>0 and TOTAL%3==0:\n",
        "            cv2.putText(frame, \"Need help\", (10, 60),cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "            response = requests.request(\"POST\", url, headers={}, data = {}) \n",
        "\t\t    #print(response.text.encode('utf8'))\n",
        "            \n",
        " \n",
        "    # show the frame\n",
        "    cv2.imshow(\"Frame\", frame)\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        " \n",
        "    # if the `q` key was pressed, break from the loop\n",
        "    if key == ord(\"q\"):\n",
        "        break\n",
        " # do a bit of cleanup\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()       \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] starting video stream thread...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b0001dc8a7d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m450\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imutils/convenience.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, width, height, inter)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# grab the image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# if both the width and height are None, then return the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ]
}